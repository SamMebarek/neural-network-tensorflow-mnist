{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification de chiffres manuscrits avec un réseau de neurones profond\n",
    "\n",
    "Ce notebook détaille la construction, l'entraînement et l'évaluation d'un réseau de neurones profond pour la classification d'images de chiffres manuscrits. Le dataset utilisé est le célèbre MNIST, souvent considéré comme le \"Hello World\" de l'apprentissage profond.\n",
    "\n",
    "### Contenu :\n",
    "1. **Préparation des données** : Chargement du dataset MNIST, mise à l'échelle des images, division en ensembles d'entraînement, de validation et de test.\n",
    "2. **Construction du modèle** : Création d'un réseau de neurones avec deux couches cachées.\n",
    "3. **Entraînement du modèle** : Utilisation de l'optimiseur Adam et de la fonction de perte `sparse_categorical_crossentropy`.\n",
    "4. **Évaluation du modèle** : Mesure de la perte et de la précision sur l'ensemble de test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les packages\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# ces ensembles de données seront stockés dans C:\\Users\\*NOM_UTILISATEUR*\\tensorflow_datasets\\...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et prétaitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfds.load charge un ensemble de données (ou le télécharge puis le charge si c'est la première fois) \n",
    "# dans notre cas, nous sommes intéressés par le MNIST; le nom de l'ensemble de données est le seul argument obligatoire\n",
    "# il y a d'autres arguments que nous pouvons spécifier, qui peuvent nous être utiles\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True nous fournira également un tuple contenant des informations sur la version, les caractéristiques, le nombre d'échantillons\n",
    "# nous utiliserons ces informations un peu plus bas et nous les stockerons dans mnist_info\n",
    "\n",
    "# as_supervised=True chargera l'ensemble de données dans une structure à 2 tuples (entrée, cible) \n",
    "# sinon, as_supervised=False renverrait un dictionnaire\n",
    "# bien sûr, nous préférons avoir nos entrées et cibles séparées \n",
    "\n",
    "# une fois que nous avons chargé l'ensemble de données, nous pouvons facilement extraire l'ensemble d'entraînement et de test avec les références intégrées\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# par défaut, TF a des ensembles d'entraînement et de test, mais pas d'ensembles de validation\n",
    "# nous devons donc le diviser nous-mêmes\n",
    "\n",
    "# nous commençons par définir le nombre d'échantillons de validation en tant que % des échantillons d'entraînement\n",
    "# c'est aussi là que nous utilisons mnist_info (nous n'avons pas besoin de compter les observations)\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# convertissons ce nombre en entier, car un float pourrait causer une erreur en cours de route\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# stockons également le nombre d'échantillons de test dans une variable dédiée (au lieu d'utiliser celle de mnist_info)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# encore une fois, nous préférerions un entier (plutôt que le float par défaut)\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# nous aimerions mettre à l'échelle nos données pour rendre le résultat plus numériquement stable\n",
    "# dans ce cas, nous préférerons simplement avoir des entrées entre 0 et 1\n",
    "# définissons une fonction appelée : scale, qui prendra une image MNIST et son label\n",
    "def scale(image, label):\n",
    "    # nous nous assurons que la valeur est un float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # comme les valeurs possibles pour les entrées sont de 0 à 255 (256 nuances différentes de gris)\n",
    "    # si nous divisons chaque élément par 255, nous obtiendrons le résultat souhaité -> tous les éléments seront entre 0 et 1 \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# la méthode .map() nous permet d'appliquer une transformation à un ensemble de données\n",
    "# nous avons déjà décidé que nous obtiendrions les données de validation à partir de mnist_train, donc \n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# enfin, nous mettons à l'échelle et regroupons les données de test\n",
    "# nous le mettons à l'échelle pour qu'il ait la même amplitude que l'entraînement et la validation\n",
    "# il n'est pas nécessaire de le mélanger, car nous ne nous entraînerons pas sur les données de test\n",
    "# il y aurait un seul batch, égal à la taille des données de test\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# mélangeons également les données\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# ce paramètre BUFFER_SIZE est ici pour les cas où nous traitons d'énormes ensembles de données\n",
    "# alors nous ne pouvons pas mélanger tout l'ensemble de données en une seule fois car nous ne pouvons pas tout mettre en mémoire\n",
    "# donc TF ne stocke que \"BUFFER_SIZE\" échantillons en mémoire à la fois et les mélange\n",
    "# si BUFFER_SIZE=1 => aucun mélange ne se produira réellement\n",
    "# si BUFFER_SIZE >= num échantillons => le mélange est uniforme\n",
    "# BUFFER_SIZE entre les deux - une optimisation computationnelle pour approximer un mélange uniforme\n",
    "\n",
    "# heureusement pour nous, il existe une méthode de mélange prête à l'emploi et nous devons simplement spécifier la taille du buffer\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# une fois que nous avons mis à l'échelle et mélangé les données, nous pouvons procéder à l'extraction réelle de l'entraînement et de la validation\n",
    "# nos données de validation seront égales à 10% de l'ensemble d'entraînement, que nous avons déjà calculé\n",
    "# nous utilisons la méthode .take() pour prendre autant d'échantillons\n",
    "# enfin, nous créons un lot avec une taille de lot égale au nombre total d'échantillons de validation\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# de même, les train_data sont tout le reste, donc nous sautons autant d'échantillons qu'il y en a dans l'ensemble de validation\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# déterminons la taille du batch\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# nous pouvons également profiter de l'occasion pour regrouper les données d'entraînement\n",
    "# cela serait très utile lorsque nous nous entraînons, car nous pourrions itérer sur les différents lots\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# regroupez les données de test\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# prend le prochain batch (c'est le seul batch)\n",
    "# car as_supervized=True, nous avons une structure à 2 tuples\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Utilise la même taille de couche cachée pour les deux couches cachées. Ce n'est pas une nécessité.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# définir la structure du modèle\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # la première couche (la couche d'entrée)\n",
    "    # chaque observation est de 28x28x1 pixels, donc c'est un tenseur de rang 3\n",
    "    # il y a une méthode pratique 'Flatten' qui prend simplement notre tenseur 28x28x1 et le transforme en un vecteur (None,) \n",
    "    # ou (28x28x1,) = (784,)\n",
    "    # cela nous permet de créer un réseau neuronal feedforward\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # couche d'entrée\n",
    "    \n",
    "    # tf.keras.layers.Dense implémente essentiellement : output = activation(dot(input, weight) + bias)\n",
    "    # il prend plusieurs arguments, mais les plus importants pour nous sont hidden_layer_size et la fonction d'activation\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1ère couche cachée\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2ème couche cachée\n",
    "    \n",
    "    # la couche finale n'est pas différente, nous nous assurons simplement de l'activer avec softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # couche de sortie\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de l'optimiseur et de la fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous définissons l'optimiseur que nous souhaitons utiliser, \n",
    "# la fonction de perte,\n",
    "# et les métriques qui nous intéressent à chaque itération\n",
    "# optimizer='adam' : L'optimiseur \"Adam\" est une méthode de descente de gradient stochastique qui est basée sur une estimation adaptative des moments de premier et second ordre.\n",
    "# loss='sparse_categorical_crossentropy' : Cette fonction de perte est utilisée pour les problèmes de classification où les classes sont mutuellement exclusives (c'est-à-dire, chaque entrée appartient exactement à une catégorie). \"Sparse\" signifie que nous utilisons une seule étiquette entière pour chaque observation pour représenter les classes (par exemple, [2, 3, 1, ...]) plutôt que des vecteurs one-hot.\n",
    "# metrics=['accuracy'] : La métrique \"accuracy\" (précision) calcule la proportion d'entrées correctement classées par le modèle.\n",
    "    \n",
    "    \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 - 3s - loss: 0.4157 - accuracy: 0.8849 - val_loss: 0.2099 - val_accuracy: 0.9410 - 3s/epoch - 6ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1782 - accuracy: 0.9477 - val_loss: 0.1515 - val_accuracy: 0.9560 - 2s/epoch - 3ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.1358 - accuracy: 0.9610 - val_loss: 0.1240 - val_accuracy: 0.9628 - 2s/epoch - 3ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.1122 - accuracy: 0.9667 - val_loss: 0.1063 - val_accuracy: 0.9695 - 2s/epoch - 3ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0944 - accuracy: 0.9717 - val_loss: 0.0918 - val_accuracy: 0.9748 - 2s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20a95eefb90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# déterminer le nombre maximal d'époques\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# nous entraînons le modèle en spécifiant :\n",
    "# les données d'entraînement\n",
    "# le nombre total d'époques\n",
    "# et les données de validation que nous avons créées nous-mêmes au format : (entrées, cibles)\n",
    "    # train_data : Les données sur lesquelles le modèle sera entraîné.\n",
    "    # epochs=NUM_EPOCHS : Le nombre d'époques est le nombre de fois que le modèle passera par l'ensemble de données d'entraînement. Dans ce cas, il passera 5 fois.\n",
    "    # validation_data=(validation_inputs, validation_targets) : Les données de validation sont utilisées pour évaluer la performance du modèle après chaque époque. Cela donne une indication de la manière dont le modèle se généralise sur des données qu'il n'a jamais vues auparavant.\n",
    "    # verbose=2 : Ce paramètre contrôle la quantité d'informations à afficher pendant l'entraînement. Avec verbose=2, l'output affichera une ligne par époque.\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir été entraîné sur les données d'entraînement et validé sur les données de validation, nous testons la puissance de prédiction finale de notre modèle en l'exécutant sur le jeu de données de test que l'algorithme n'a JAMAIS vu auparavant.\n",
    "\n",
    "Il est très important de réaliser que la manipulation des hyperparamètres sur-entraine le jeu de données de validation.\n",
    "\n",
    "Le test est l'instance finale absolue. On ne doit pas tester avant d'avoir complètement ajusté votre modèle.\n",
    "\n",
    "Si On ajuste notre modèle après le test, nous commencerons à sur-entrainer le jeu de données de test, ce qui en annulera l'objectif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1124 - accuracy: 0.9675\n",
      "Perte lors du test : 0.11. Précision lors du test : 96.75%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "# model.evaluate(test_data) : Cette fonction évalue la performance du modèle sur les données de test. Elle renvoie la perte (loss) et la précision (accuracy) du modèle sur ces données.\n",
    "# print('Perte lors du test : {0:.2f}. Précision lors du test : {1:.2f}%'.format(test_loss, test_accuracy*100.)) : Cette ligne affiche la perte et la précision du modèle sur les données de test. Le formatage {0:.2f} et {1:.2f} est utilisé pour afficher les nombres avec deux décimales.\n",
    "\n",
    "\n",
    "\n",
    "print('Perte lors du test : {0:.2f}. Précision lors du test : {1:.2f}%'.format(test_loss, test_accuracy*100.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant le modèle initial et les hyperparamètres donnés dans ce notebook, la précision finale du test devrait être d'environ 97%.\n",
    "\n",
    "Chaque fois que le code est exécuté à nouveau, nous obtenons une précision différente car les lots sont mélangés, les poids sont initialisés différemment, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
